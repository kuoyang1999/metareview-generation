# This file makes the 'attn' directory a Python package.
# Again, can be left empty or can contain imports from llama_attn_replace_sft if desired.

"""
Attention Mechanism Replacements
--------------------------------
This package provides modules that patch model attention implementations
for improved performance or functionality (e.g., flash attention).
"""